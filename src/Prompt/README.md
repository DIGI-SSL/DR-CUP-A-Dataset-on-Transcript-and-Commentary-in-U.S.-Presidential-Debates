# Simplified Prompt Format for<br/> Commentary Understanding and Planning
| Prompt Component | Component Definition | Prompt Example |
|:--|:--|:--|
|Commentary and Transcript Input <br/>(Label Prediction)|Commentary from a commentator regarding the U.S. presidential election.<br/>The transcript is included if available.|This is a commentary made by a commentator regarding the U.S. presidential election: {***commentary***}.<br/> This is the corresponding debate transcript: {***transcript***}. <br/>"***NO***" means no transcript is available.|
|Transcript-Only Input<br/>(Label Planning)|This experiment generates a commentary label based on the U.S. presidential debate transcript. <br/>If the transcript is "***NO***", previous transcripts are merged into a continuous passage for labeling.|This is a transcript of a U.S. presidential election debate: {***transcript***}.<br/>
|Label Definition| 11 labels classify the commentary, including <br/>**Key Summary**, **Fact-Checking**, **Commentator’s Opinion**, etc.<br/> Definitions are provided.|Refer to label definitions: <br/>**Key Summary**, **Fact-Checking**, etc.|
|Formatted Output| Identify the most likely label based on the transcript and commentary.|Label: **Key Summary**.|
|Example<br/> (Commentary Understanding)|In Few-Shot experiments, this part is used to provide LLMs with the correct corresponding relationships, serving as the basis for predicting the correct label for the commentary. <br/>(*Note: Zero-Shot experiments will not include examples.*)|Commentary: {***Insert Commentary***}<br/>Label: {***Corresponding Label***}|
|Example<br/> (Commentary Planning)| In Few-Shot experiments, this part is used to inform LLMs of the correct correspondence between the commentary and the transcript. However, the model will ultimately only receive the transcript and use it as the basis for determining which label of commentary to generate.<br/>(*Note: Zero-Shot experiments will not include examples.*)|Commentary: {***Insert Commentary***}<br/>Transcripts: {***Insert Transcripts***}<br/>Label: {***Corresponding Label***}
# Simplified Prompt Format for<br/> Commentary Generation、Choose and Scoring
| Prompt Component  | Component Definition | Prompt Example |
|:--|:--|:--|
|Transcripts and labels with definitions Input<br/>(Generation)|Manually annotated labels aligning Bloomberg’s commentary with the 2023–2024 presidential-election debate transcripts.|This is a transcript from the 2023–2024 U.S. presidential election debates: <br/>{***Transcripts***}. <br/>Please study the transcript carefully and match it to the following labels and definitions: <br/>{***Labels & Definitions***}.|
|Specify the role to play and the formatted output<br/>(Generation)|Have the LLM generate the corresponding commentary from a professional commentator’s perspective and format the output for easy reading.|From the perspective of a professional commentator, generate an English commentary that aligns with the label definitions above and relates to the transcript. <br/>Provide only the commentary text you create, and place the heading “***Commentary***:” before it for easy reading.|
|Transcripts and Commentary Inputs<br/>(Choose)|The commentaries generated by the four LLMs used in the Generation experiment (Gemini 2.0, Claude 3.5 Sonnet, GPT-4o, and DeepSeek R1-1776) were aligned with the corresponding transcript segments, along with the expert-written commentaries. These aligned commentaries were then evaluated by three LLMs (Gemini 2.0, Claude 3.5 Sonnet, and GPT-4o), each tasked with selecting the best commentary—or opting not to select any if none were deemed suitable.|Below is a transcript excerpt from the 2023–2024 U.S. Presidential Debate: <br/>"***Transcripts***"<br/>Next, there are five commentaries:<br/>Type:"**Summary/Commentary**","***Commentary 1***"<br/>Type:"**Summary/Commentary**","***Commentary 2***"<br/>Type:"**Summary/Commentary**","***Commentary 3***"<br/>Type:"**Summary/Commentary**","***Commentary 4***"<br/>Type:"**Summary/Commentary**","***Commentary 5***"<br/>|
|Formatted Output<br/>(Choose)|Have the LLM select the commentary it considers the best and output the result in the specified format.|Please select the commentary you are most satisfied with.(If you believe none of the commentaries are appropriate, you may choose not to select any and respond with "***No***".)<br/>Just provide your single best choice.|
|Transcripts and Commentary Inputs<br/>(Scoring)|Align the commentaries generated by the four LLMs used in the Generation experiment (Gemini 2.0, Claude 3.5 Sonnet, GPT-4o, and DeepSeek R1-1776) and the expert commentary with the corresponding transcript content, one by one.|Below is a transcript excerpt from the 2023–2024 U.S. Presidential Debate: <br/>"***Transcripts***"<br/>Next, there is a commentary:<br/>Type:"**Summary/Commentary**","***Commentary 1***"|
|Scoring Criteria|<br/>**Evidence Relevance**: Evaluates how well the summary aligns with the content of the transcript. Rated on a scale from 1 (low relevance) to 5 (high relevance).<br/>**Style**: Assesses the writing style of the summary, from unacceptable (1) to acceptable (5).<br/>**Factual Consistency**: Measures the consistency between the content described in the summary and the actual transcript. Rated from 1 (low consistency) to 5 (high consistency).<br/>**Comprehensibility**: Evaluates how well the summary helps a general audience understand the content of the transcript. Scored from 1 (hard to understand) to 5 (easy to understand).<br/>**Importance**: Assesses how important the commentary is for the general public—that is, how much the issue discussed in the commentary affects everyday people. Rated from 1 (low impact) to 5 (high impact).<br/>**Comprehensibility**: Evaluates how easy the commentary is for a general audience to understand, and how clearly the commentator conveys their point. Scored from 1 (unclear) to 5 (very clear).<br/>**Predictability**: Measures whether the content of the commentary is expected or surprising to a general audience. Scored from 1 (highly expected) to 5 (highly unexpected).<br/>**Plausibility**: Assesses the likelihood of the scenario described in the commentary occurring. The more plausible, the higher the score, from 1 to 5.<br/>**Credibility**: Evaluates how trustworthy the commentary is. The more credible it is, the higher the score, ranging from 1 (not credible) to 5 (very credible).|<br/>**Summary**<br/>**Evidence Relevance**:{***Definition***}<br/>**Style**:{***Definition***}<br/>**Factual Consistency**:{***Definition***}<br/>**Comprehensibility**:{***Definition***}<br/><br/>**Commentary**<br/>**Importance**:{***Definition***}<br/>**Comprehensibility**:{***Definition***}<br/>**Predictability**:{***Definition***}<br/>**Plausibility**:{***Definition***}<br/>**Credibility**:{***Definition***}|
|Formatted Output<br/>(Scoring)|Enable LLMs to generate scores in a specific format to facilitate subsequent data processing and analysis.|Summary :<br/>**Evidence Relevance**: 1-5<br/>**Style**: 1-5<br/>**Factual Consistency**: 1-5<br/>**Comprehensibility**: 1-5<br/><br/>Commentary: <br/>**Importance**: 1-5<br/>**Comprehensibility**: 1-5<br/>**Predictability**: 1-5<br/>**Plausibility**: 1-5<br/>**Credibility**: 1-5

